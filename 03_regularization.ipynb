{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Dropout 正则化\n",
    "\n",
    "实现了对权重的正则化，引入`Dropout`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 处理一下数据集作为输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Logistic Regression` 逻辑回归\n",
    "- `Regulation` 正则化\n",
    "- `Accurancy` 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  lambda_regu = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + lambda_regu * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 21.915903\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 8.8%\n",
      "Minibatch loss at step 500: 2.518258\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 1000: 1.546754\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 1500: 1.432281\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2000: 0.832008\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2500: 0.757161\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 3000: 0.770034\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为正则化参数不知道该如何选择，所以进行测试，发现`1e-3`比较好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvlJK+RBN+kop5UM06SullA/5/4YKrNy+kLwuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1376275a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三层网络\n",
    "- 正则化\n",
    "- `Accurancy` 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  lambda_regu = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    lambda_regu * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1), weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1), weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 695.143860\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 34.3%\n",
      "Minibatch loss at step 500: 191.012466\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 1000: 113.954475\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1500: 69.089806\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 2000: 41.309517\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.169491\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3000: 15.394077\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.5%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为正则化参数不知道该如何选择，所以进行测试，发现`1e-3`比较好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEMCAYAAADUEk3/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13e3706cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将训练数据限制在一个小范围内，发现训练集准确率很高，测试集和验证集就很差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 637.683899\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 32.9%\n",
      "Minibatch loss at step 500: 190.933517\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1000: 115.792572\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1500: 70.222961\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 2000: 42.587135\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 2500: 25.827457\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 3000: 15.664238\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.6%\n",
      "Test accuracy: 82.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "a_few_batches = 5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (a_few_batches * batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu: 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三层网络\n",
    "- `Dropout`\n",
    "- `Accurancy` 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  lambda_regu = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  hidden_drop = tf.nn.dropout(hidden, keep_prob)\n",
    "  logits = tf.matmul(hidden_drop, weights_2) + biases_2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    lambda_regu * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_hidden_prob = tf.nn.dropout(valid_hidden, 1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_prob, weights_2) + biases_2)\n",
    "  # it is also ok\n",
    "  #valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights_2) + biases_2)\n",
    "    \n",
    "  test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_hidden_drop = tf.nn.dropout(test_hidden, 1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_drop, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 789.955505\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 35.8%\n",
      "Minibatch loss at step 500: 189.951035\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1000: 114.906570\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1500: 68.822655\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 2000: 41.219700\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.150171\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3000: 15.422319\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.6%\n",
      "Test accuracy: 92.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu: 1e-3, keep_prob: 0.75}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将训练数据限制在一个小范围内，发现训练集准确率很高，测试集和验证集就很差\n",
    "- 但是加了`Dropout`，就好多了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 701.468994\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 31.8%\n",
      "Minibatch loss at step 500: 190.662994\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 1000: 115.648041\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 1500: 70.145401\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 2000: 42.542118\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 2500: 25.800838\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 3000: 15.647261\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 78.0%\n",
      "Test accuracy: 85.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "a_few_batches = 5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (a_few_batches * batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu: 1e-3, keep_prob: 0.75}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 四层网络\n",
    "- `Adagrad Optimizer`\n",
    "- 正则化\n",
    "- `Dropout`\n",
    "- 参数初始化真的非常影响结果\n",
    "- https://github.com/rndbrtrnd/udacity-deep-learning/blob/master/3_regularization.ipynb 达到了96%\n",
    "- 官方说可以达到97&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden1_nodes = 1024\n",
    "num_hidden2_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  lambda_regu = tf.placeholder(tf.float32)\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden1_nodes], stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_hidden1_nodes]))\n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden1_nodes, num_hidden2_nodes], stddev=np.sqrt(2.0 / num_hidden1_nodes)))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_hidden2_nodes]))  \n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden2_nodes, num_labels], stddev=np.sqrt(2.0 / num_hidden2_nodes)))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  hidden1_drop = tf.nn.dropout(hidden1, keep_prob)\n",
    "  hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights_2) + biases_2)\n",
    "  hidden2_drop = tf.nn.dropout(hidden2, keep_prob)\n",
    "  logits = tf.matmul(hidden2_drop, weights_3) + biases_3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "    lambda_regu * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdagradOptimizer(0.3).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  valid_hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "  valid_hidden2 = tf.nn.relu(tf.matmul(valid_hidden1, weights_2) + biases_2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden2, weights_3) + biases_3)\n",
    "    \n",
    "  test_hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "  test_hidden2 = tf.nn.relu(tf.matmul(test_hidden1, weights_2) + biases_2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden2, weights_3) + biases_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.573497\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 38.0%\n",
      "Minibatch loss at step 500: 1.000830\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1000: 0.822930\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1500: 0.890118\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 2000: 0.625403\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2500: 0.634579\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 0.696254\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3500: 0.676903\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4000: 0.736637\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 4500: 0.839940\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 5000: 0.660201\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 5500: 0.506929\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 6000: 0.739235\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 6500: 0.621628\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 7000: 0.632907\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 7500: 0.678495\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 8000: 0.691816\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 8500: 0.652273\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 9000: 0.662610\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 9500: 0.651666\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 10000: 0.689943\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 10500: 0.716481\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 11000: 0.729964\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 11500: 0.596099\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 12000: 0.860799\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 12500: 0.536089\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 13000: 0.943495\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 13500: 0.833625\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 14000: 0.642545\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 14500: 0.754652\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 15000: 0.637490\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 15500: 0.516181\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 16000: 0.668106\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 16500: 0.557820\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 17000: 0.556003\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 17500: 0.627893\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 18000: 0.484027\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, lambda_regu: 1e-3, keep_prob: 0.5}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
