{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 数据处理成`[batch_size, image_size, image_size, channels]`\n",
    "- 标签处理成`one hot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 两层卷积层，其中卷积层步长为2\n",
    "- 特征映射的数目均为64\n",
    "- 准确率 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 14, 14, 16]\n",
      "[16, 7, 7, 16]\n",
      "[10000, 14, 14, 16]\n",
      "[10000, 7, 7, 16]\n",
      "[10000, 14, 14, 16]\n",
      "[10000, 7, 7, 16]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(shape)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(shape)\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.809623\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 8.2%\n",
      "Minibatch loss at step 50: 1.407817\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 100: 1.579872\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 64.3%\n",
      "Minibatch loss at step 150: 0.726914\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 200: 0.895602\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 250: 0.702833\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 300: 0.887109\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 350: 0.459326\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 400: 0.649843\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 450: 0.944354\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 500: 0.571693\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 550: 0.542660\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 600: 0.758760\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 650: 1.117504\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 700: 0.447662\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 750: 0.342475\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 800: 0.085199\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 850: 0.630253\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 900: 0.674020\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 950: 0.727466\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1000: 0.934118\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.7%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 直接改成加上池化层效果还是88%左右\n",
    "- 所以改成了特征映射分别为`32`，`64`，全连接隐藏节点`256`个；隐藏节点`1024`个效果非常差。\n",
    "- 准确率： 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 14, 14, 32]\n",
      "[16, 7, 7, 64]\n",
      "[10000, 14, 14, 32]\n",
      "[10000, 7, 7, 64]\n",
      "[10000, 14, 14, 32]\n",
      "[10000, 7, 7, 64]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "num_hidden = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth1]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth1, depth2], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth2, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(tf.nn.max_pool(conv + layer1_biases, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME'))\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(shape)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(tf.nn.max_pool(conv + layer2_biases, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME'))\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    print(shape)\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 9.630028\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.061450\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 60.5%\n",
      "Minibatch loss at step 100: 1.853973\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 150: 0.642839\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 200: 0.505073\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 250: 0.570606\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 300: 0.516718\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 350: 0.508198\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 400: 0.572958\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 450: 0.759367\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 500: 0.613860\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 550: 0.363899\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 600: 0.696997\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 650: 0.929037\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 700: 0.511295\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 750: 0.401853\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 800: 0.040452\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 850: 0.733334\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 900: 0.713554\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 950: 0.434209\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 0.847203\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 90.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- 类似LeNet5，然而并不是，没有Dropout，没有正则化\n",
    "- 标准LeNet5\n",
    "  ![标准LeNet5](http://7xsbsy.com1.z0.glb.clouddn.com/NNML__5__8.png)\n",
    "- 一个简单的LeNet\n",
    "  ![一个简单的LeNet](http://deeplearning.net/tutorial/_images/mylenet.png)\n",
    "- 准确率： 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "constant_bias = 0.1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.constant(constant_bias, shape=[depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(constant_bias, shape=[depth]))\n",
    "  size3 = ((image_size - patch_size + 1) // 2 - patch_size + 1) // 2\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [size3 * size3 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(constant_bias, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(constant_bias, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # C1 input 28 x 28\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # S2 input 24 x 24\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # C3 input 12 x 12\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    # S4 input 8 x 8\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # F6 input 4 x 4\n",
    "    shape = pool4.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.293002\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.5%\n",
      "Minibatch loss at step 50: 1.665669\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 34.8%\n",
      "Minibatch loss at step 100: 1.755066\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 51.7%\n",
      "Minibatch loss at step 150: 0.798242\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 200: 1.024715\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 250: 0.700513\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 300: 0.947052\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 350: 0.505861\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 400: 0.859947\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 450: 1.171692\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 500: 0.785399\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 550: 0.665577\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.1%\n",
      "Minibatch loss at step 600: 1.157756\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 650: 0.997647\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 700: 0.588002\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 750: 0.542212\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 800: 0.095129\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 850: 0.786316\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 900: 0.587449\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 950: 0.784695\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 1000: 0.937928\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1050: 1.240897\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 1100: 0.770662\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1150: 0.607396\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1200: 0.670522\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 1250: 0.143764\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 1300: 0.573640\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1350: 0.893559\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1400: 0.457182\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1450: 0.262059\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1500: 0.654835\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 1550: 0.386479\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 1600: 0.464813\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 1650: 0.405664\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 1700: 0.587145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 1750: 0.903549\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1800: 0.361418\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1850: 0.241584\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1900: 0.617919\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1950: 0.267315\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2000: 0.468031\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2050: 0.594292\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2100: 0.911490\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2150: 0.399290\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2200: 0.405056\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2250: 0.325055\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2300: 0.105119\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2350: 0.992349\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 2400: 0.106494\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2450: 0.657001\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2500: 0.649624\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 2550: 0.624141\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2600: 0.149770\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2650: 0.451225\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 2700: 0.616081\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2750: 0.452393\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2800: 1.350245\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 2850: 0.294952\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 2900: 0.606306\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 2950: 0.892120\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3000: 0.555227\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3050: 0.923672\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 3100: 0.300386\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3150: 0.214271\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3200: 0.843607\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3250: 0.293129\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3300: 0.153495\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3350: 0.349763\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3400: 0.813751\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3450: 0.829772\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3500: 0.344616\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3550: 0.304942\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3600: 0.201335\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3650: 0.262818\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3700: 0.847595\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3750: 0.720075\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3800: 0.245542\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 3850: 0.722887\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3900: 0.301363\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 3950: 0.104979\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 4000: 0.794806\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4050: 0.378826\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 4100: 0.642311\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4150: 0.297453\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4200: 0.505077\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 4250: 0.682848\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 4300: 0.139214\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4350: 0.590529\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4400: 0.602980\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4450: 0.250459\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 4500: 1.354738\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 4550: 0.588492\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4600: 0.183664\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 4650: 0.457863\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4700: 0.786776\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4750: 0.432912\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 4800: 0.384129\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 4850: 0.702200\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4900: 0.236797\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 4950: 0.244601\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 5000: 0.181818\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 5050: 0.390000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 5100: 0.057622\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 5150: 0.169089\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 5200: 0.994133\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 5250: 0.830926\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 5300: 0.073595\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 5350: 0.405424\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 5400: 0.604830\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 5450: 0.650985\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 5500: 0.989509\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 5550: 0.201624\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 5600: 0.482799\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5650: 0.330211\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 5700: 0.430781\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5750: 0.068132\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 5800: 0.450756\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 5850: 0.294676\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 5900: 0.613295\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 5950: 0.326537\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6000: 0.622496\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6050: 0.395329\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 6100: 0.627183\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6150: 0.125658\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 6200: 0.678826\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6250: 0.604138\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 6300: 0.310948\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 6350: 0.465902\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 6400: 0.348956\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6450: 0.412578\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 6500: 0.282089\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 6550: 0.381648\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6600: 0.351274\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 6650: 0.091781\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 6700: 0.178827\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6750: 0.307258\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6800: 0.230458\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 6850: 0.349656\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 6900: 0.197533\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 6950: 0.252413\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 7000: 0.602625\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 7050: 0.867869\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7100: 1.926765\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 7150: 0.571686\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 7200: 0.261598\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7250: 0.433971\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7300: 0.441092\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 7350: 0.500416\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 7400: 0.481449\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7450: 0.297539\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 7500: 0.149864\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7550: 0.389714\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 7600: 0.274287\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 7650: 0.173878\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 7700: 0.588089\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7750: 0.555682\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 7800: 0.539964\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7850: 0.904858\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 7900: 1.080393\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7950: 0.603856\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 8000: 0.331987\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 8050: 0.159842\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 8100: 0.230501\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 8150: 0.149548\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 8200: 0.300768\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 8250: 0.679744\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8300: 0.045953\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 8350: 0.710171\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 8400: 0.655801\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 8450: 0.271493\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8500: 0.502897\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8550: 0.374316\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8600: 0.178748\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8650: 0.356316\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8700: 0.508865\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 8750: 0.077147\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 8800: 0.087805\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8850: 0.399738\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 8900: 0.563917\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 8950: 0.669359\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 9000: 0.445666\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 9050: 0.416837\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 9100: 0.546519\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 9150: 0.430086\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 9200: 0.165215\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 9250: 0.343510\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 9300: 0.211359\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 9350: 0.795027\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 9400: 0.090717\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 9450: 0.156312\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 9500: 0.269369\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 9550: 0.995477\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 9600: 0.156292\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 9650: 0.322254\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 9700: 0.673918\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 9750: 0.719395\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 9800: 0.217645\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 9850: 0.445400\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 9900: 0.482430\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 9950: 0.551578\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 10000: 0.841434\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 10050: 0.511008\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 10100: 0.019363\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 10150: 0.700590\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 10200: 0.139846\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 10250: 0.164875\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 10300: 0.685329\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10350: 0.964226\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 10400: 0.050386\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 10450: 0.582487\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10500: 0.261600\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 10550: 0.257064\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10600: 0.288924\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 10650: 0.976007\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 10700: 0.390949\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 10750: 0.330356\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 10800: 0.100335\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 10850: 0.067216\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 10900: 0.309659\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 10950: 0.299852\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 11000: 0.115368\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 11050: 0.351672\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 11100: 0.781743\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 11150: 0.971414\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 11200: 0.205287\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 11250: 0.113921\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 11300: 0.745252\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11350: 0.339352\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 11400: 0.689349\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 11450: 0.875266\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11500: 0.835935\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 11550: 0.258698\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11600: 0.497018\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11650: 0.260323\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 11700: 0.248769\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 11750: 0.211325\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 11800: 0.739819\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 11850: 0.246695\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 11900: 0.221194\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 11950: 0.277080\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 12000: 0.379806\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 12050: 0.344741\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 12100: 0.262236\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 12150: 0.556376\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 12200: 0.692382\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 12250: 0.825997\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12300: 0.463932\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12350: 0.006888\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 12400: 0.523905\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12450: 0.112603\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 12500: 0.379549\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 12550: 0.479560\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12600: 0.619008\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 12650: 0.399956\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 12700: 0.460512\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 12750: 0.201754\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 12800: 0.140721\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12850: 0.623410\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 12900: 0.413645\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 12950: 0.166363\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 13000: 0.504627\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 13050: 0.064794\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 13100: 0.158819\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 13150: 0.460558\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 13200: 0.442321\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 13250: 0.375492\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 13300: 0.299823\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 13350: 0.382635\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 13400: 0.198472\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 13450: 0.178615\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 13500: 0.695640\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 13550: 1.026122\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 13600: 0.402609\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 13650: 0.207717\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 13700: 0.579750\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 13750: 0.050460\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 13800: 0.224925\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 13850: 0.388616\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 13900: 0.348030\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 13950: 0.129713\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 14000: 0.222744\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 14050: 0.679417\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 14100: 0.621821\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 14150: 0.573061\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 14200: 0.443093\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 14250: 0.136166\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14300: 0.360795\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 14350: 0.101824\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 14400: 0.171793\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 14450: 0.052361\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14500: 0.152115\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 14550: 0.193999\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 14600: 0.165753\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 14650: 0.755019\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 14700: 0.537448\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 14750: 0.250154\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14800: 0.731289\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 14850: 0.438744\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 14900: 0.112678\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 14950: 0.209266\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15000: 0.208303\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15050: 0.621168\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 15100: 0.161676\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15150: 0.344422\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15200: 0.810183\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 15250: 0.428230\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 15300: 0.436968\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15350: 0.190730\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 15400: 0.122093\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15450: 0.466728\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 15500: 1.008217\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15550: 0.636377\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 15600: 0.261964\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 15650: 0.361396\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 15700: 0.894136\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 15750: 0.156132\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15800: 0.476300\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15850: 0.121556\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15900: 0.275758\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 15950: 0.283701\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 16000: 0.746459\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16050: 0.150883\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 16100: 0.059758\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 16150: 0.797393\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 16200: 0.380634\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 16250: 0.167149\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 16300: 0.878963\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 16350: 0.132563\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16400: 0.704488\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 16450: 0.231331\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 16500: 0.559396\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 16550: 0.252577\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 16600: 0.259333\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 16650: 0.381114\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 16700: 0.511589\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 16750: 0.029802\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16800: 0.485273\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16850: 0.240965\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16900: 0.383821\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 16950: 0.689730\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17000: 0.435235\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 17050: 0.243403\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 17100: 0.230172\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 17150: 1.028382\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 17200: 0.247084\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 17250: 0.318690\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17300: 0.199171\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17350: 0.212364\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 17400: 0.401539\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 17450: 0.036041\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 17500: 0.593462\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 17550: 0.015406\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 17600: 0.060683\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17650: 0.237917\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17700: 0.517211\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 17750: 0.557285\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 17800: 0.464670\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 17850: 0.527984\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 17900: 0.283290\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 17950: 0.219502\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 18000: 0.346167\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18050: 0.414118\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 18100: 0.336475\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18150: 0.156785\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 18200: 0.117417\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18250: 0.036484\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18300: 1.037082\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 18350: 0.117238\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18400: 0.231460\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 18450: 0.095652\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 18500: 0.159541\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 18550: 0.261498\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18600: 0.665140\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18650: 0.322988\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 18700: 0.046090\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18750: 0.786895\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 18800: 0.679143\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 18850: 0.499618\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 18900: 0.390567\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 18950: 0.317421\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19000: 0.369968\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19050: 0.078867\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 19100: 0.177833\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 19150: 0.065846\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 19200: 0.209352\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19250: 0.455219\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 19300: 0.264301\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19350: 0.318611\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19400: 0.303967\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19450: 0.378286\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19500: 0.319870\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19550: 0.416141\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19600: 0.361318\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 19650: 0.030205\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19700: 0.514473\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19750: 0.363475\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19800: 0.363696\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 19850: 0.483819\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 19900: 0.118006\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19950: 0.392590\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 20000: 0.442784\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.9%\n",
      "Test accuracy: 94.2%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCK+8A0bcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0912d0550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x1 = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      acc1 = accuracy(predictions, batch_labels)\n",
    "      print('Minibatch accuracy: %.1f%%' % acc1)\n",
    "      acc2 = accuracy(valid_prediction.eval(), valid_labels)\n",
    "      print('Validation accuracy: %.1f%%' % acc2)\n",
    "      x1.append(step)\n",
    "      y1.append(acc1)\n",
    "      y2.append(acc2)\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  plt.plot(x1, y1, label='train')\n",
    "  plt.plot(x1, y2, label='valid')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
